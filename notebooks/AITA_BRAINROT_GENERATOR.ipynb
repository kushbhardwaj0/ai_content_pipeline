# -*- coding: utf-8 -*-
"""
notes:

use t4 gpu

remember to import the aitah_stories.json file

remember to import the brain_number_trimmed.mp4 file

enter the api key when prompted on cell 9

# Training/generating the text
"""

"""X_LLM_Trainer.ipynb"""

# Install required libraries
!pip install datasets sympy wandb
!pip install --no-cache-dir bitsandbytes
!pip install unsloth  # Install Unsloth

import torch
from transformers import TrainingArguments, Trainer, AutoTokenizer
from datasets import load_dataset
import wandb
from unsloth import FastLanguageModel  # Import Unsloth
from peft import PeftModel  # Import PeftModel for loading saved LoRA adapters

from google.colab import drive
drive.mount('/content/drive/')

stories = '/content/drive/MyDrive/Personal_stuff/brainrot/aitah_stories.json'

# Initialize Wandb
wandb.login()
wandb.init(
    project="demo-yt-video",
    config={
        "learning_rate": 5e-5,
        "architecture": "SmolLM2-1.7B-Instruct",
        "dataset": stories,
        "epochs": 3,
    }
)
#eca726d7d8a4a45d48548d75e0057a904c4506ed

# Load the correct tokenizer
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")

# Load the model with Unsloth
model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
model, _ = FastLanguageModel.from_pretrained(
    model_name=model_name,
    dtype=torch.float16,  # Explicitly set dtype to torch.float16
    load_in_4bit=True,  # Use 4-bit quantization for memory efficiency
)

# Load the dataset
dataset = load_dataset("json", data_files={"train": stories}, split="train")
print("Dataset size:", len(dataset))

# Split the dataset
train_test_split = dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split["train"]
eval_dataset = train_test_split["test"]

# Tokenization function
def tokenize_function(examples):
    combined_texts = [f"{prompt} {completion}" for prompt, completion in zip(examples["instruction"], examples["output"])]
    tokenized = tokenizer(combined_texts, truncation=True, max_length=512, padding="max_length", return_tensors="pt")
    tokenized["labels"] = tokenized["input_ids"].clone()
    return tokenized

# Tokenize the datasets
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# Print a sample tokenized output for debugging
sample_output = tokenized_train_dataset[0]
print("Sample tokenized output:", sample_output)
print("Decoded sample output:", tokenizer.decode(sample_output["input_ids"]))

model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Adjust rank for efficiency
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]  # Ensure correct target modules
)

# Step 3: Clear GPU Cache to Free Memory
print("Clearing GPU cache...")
torch.cuda.empty_cache()

# Step 4: Define Training Arguments
print("Setting up training arguments...")
training_args = TrainingArguments(
    output_dir="/content/smollm2_finetuned",  # Adjusted output directory
    num_train_epochs=10,  # Using epochs instead of steps
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
    fp16=True,  # Enable mixed precision training
    logging_steps=10,
    save_steps=30,  # Checkpoint saving frequency
    #evaluation_strategy="steps",
    eval_steps=30,
    save_total_limit=3,  # Keep the latest 3 checkpoints
    learning_rate=3e-5,
    logging_dir="./logs",
    report_to="wandb",
    run_name="SmolLM2_FineTuning_Experiment"
)

# Step 5: Initialize the Trainer
print("Initializing trainer...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
)

trainer.train()
trainer.save_model("/content/smollm2_finetuned/checkpoint-46")

!pip install --upgrade transformers peft

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# Load the base model
base_model = AutoModelForCausalLM.from_pretrained(
    "HuggingFaceTB/SmolLM2-1.7B-Instruct",
    torch_dtype=torch.float16
).to("cuda")

# Load the fine-tuned adapter using PeftModel
model = PeftModel.from_pretrained(
    base_model,  # Base model
    "/content/smollm2_finetuned/checkpoint-46",  # Path to the fine-tuned adapter
    torch_dtype=torch.float16
).to("cuda")

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")

# Define a better prompt for generating political tweets
messages = [
    {"role": "system", "content": "You are a reddit user making engaging and drama-filled content. Write in first person point of view as if you are writing on a social media platform."},
    {"role": "user", "content": "Write a short story asking reddit about whether you are an asshole for selling a TV that my brother uses."}
]

# Format the prompt properly
input_text = tokenizer.apply_chat_template(messages, tokenize=False)

# Encode the input with an attention mask
inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to("cuda")

# Set the `max_seq_length` attribute for the model
# Assuming your model's config has `max_position_embeddings`,
# otherwise, you might need to adjust this to the actual max sequence length.
model.base_model.config.max_seq_length = model.base_model.config.max_position_embeddings

!pip install git+https://github.com/huggingface/transformers.git
!pip install accelerate

# Generate text with higher randomness
outputs = model.generate(
    inputs["input_ids"],  # Ensure input tensor is passed correctly
    attention_mask=inputs["attention_mask"],  # Set attention mask explicitly
    # max_length=100,  # Maximum length of the generated sequence
    max_new_tokens=350,  # Keep it reddit-sized
    temperature=1.0,  # Increase randomness for more diverse tweets
    top_p=0.9,  # Use nucleus sampling for variety
    do_sample=True  # Enables non-deterministic sampling
)

# Decode and print the generated tweet
generated_tweet = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Tweet:", generated_tweet)

start_index = generated_tweet.find("assistant") + len("assistant")

# Extract the content between **Story** and <|end_of_text|>
output_string_cut = generated_tweet[start_index:].strip()
print(output_string_cut)

"""#Text to speech"""

!pip install gtts
!pip install playsound

from gtts import gTTS

def text_to_speech(text, filename="input_audio.mp3"):
  """Converts text to speech and saves it as an MP3 file."""

  tts = gTTS(text=text, lang='en')  # 'en' for English
  tts.save(filename)

text_to_speech(output_string_cut)

# speeding it up
!pip install pydub
from pydub import AudioSegment


# Open the saved audio file
sound = AudioSegment.from_mp3("/content/input_audio.mp3")

# Increase playback speed (adjust the value for desired speed)
speed_up_sound = sound.speedup(playback_speed=1.3)  # 1.5x faster

# Save the sped-up audio (optional)
speed_up_sound.export("output.mp3", format="mp3")

"""# GENERATING VIDEO/SUBTITLES"""

# Install ffmpeg
!apt-get update
!apt-get install -y ffmpeg

# Install Whisper and its dependencies
!pip install -q git+https://github.com/openai/whisper.git

# Install pysrt for handling subtitle files
!pip install -q pysrt

import whisper
import pysrt
import subprocess
import os

# File paths
audio_file = "/content/output.mp3"
video_file = "/content/drive/MyDrive/Personal_stuff/brainrot/brain_number_trimmed_ss.mp4"
srt_file = "subtitles.srt"
output_video = "final_video.mp4"

# Load Whisper model
model_audio = whisper.load_model("base")  # Options: "tiny", "small", "medium", "large"

# Transcribe the audio
print("Transcribing audio...")
result = model_audio.transcribe(audio_file, task='transcribe')

# Extract segments
segments = result['segments']

# Helper function to convert seconds to SubRipTime
def seconds_to_subriptime(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    milliseconds = int(round((seconds - int(seconds)) * 1000))
    return pysrt.SubRipTime(hours=hours, minutes=minutes, seconds=secs, milliseconds=milliseconds)

# Create SRT file
subs = pysrt.SubRipFile()
for i, segment in enumerate(segments, start=1):
    start_time = seconds_to_subriptime(segment['start'])
    end_time = seconds_to_subriptime(segment['end'])
    text = segment['text'].strip().replace('--', '-')
    sub = pysrt.SubRipItem(index=i, start=start_time, end=end_time, text=text)
    subs.append(sub)

subs.save(srt_file, encoding='utf-8')
print(f"Subtitles saved to {srt_file}")

# Function to get duration of a media file using ffprobe
def get_media_duration(file_path):
    result = subprocess.run(
        ['ffprobe', '-v', 'error', '-show_entries',
         'format=duration', '-of',
         'default=noprint_wrappers=1:nokey=1', file_path],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT
    )
    return float(result.stdout)

# Get durations
audio_duration = get_media_duration(audio_file)
video_duration = get_media_duration(video_file)
print(f"Audio duration: {audio_duration} seconds")
print(f"Video duration: {video_duration} seconds")

# Determine the shorter duration
shortest_duration = min(audio_duration, video_duration)
print(f"Shortest duration: {shortest_duration} seconds")

# FFmpeg command to burn subtitles, set audio, and trim video
command = [
    'ffmpeg',
    '-y',  # Overwrite output file if it exists
    '-i', video_file,           # Input video
    '-i', audio_file,           # Input audio
    '-vf', f"subtitles={srt_file}:force_style='FontName=Arial,FontSize=20,PrimaryColour=&HFFFFFF&'",  # Burn subtitles
    '-c:v', 'libx264',          # Video codec
    '-c:a', 'aac',              # Audio codec
    '-b:a', '192k',             # Audio bitrate
    '-t', str(shortest_duration),  # Set the duration explicitly
    output_video
]

print("Burning subtitles and setting audio...")
# Execute the FFmpeg command
process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

# Check if FFmpeg executed successfully
if process.returncode != 0:
    print("FFmpeg encountered an error:")
    print(process.stderr)
else:
    print(f"Final video created: {output_video}")

# Verify that the output video exists
if os.path.exists(output_video):
    print(f"{output_video} has been successfully created.")
else:
    print(f"Error: {output_video} was not created.")

# Combine final_video.mp4 (video) with input_audio.mp3 (audio)
new_video_file = 'new_final_video.mp4'

ffmpeg_command = [
    'ffmpeg',
    '-y',  # Overwrite output file if it exists
    '-i', 'final_video.mp4',
    '-i', 'output.mp3',
    '-c:v', 'copy',   # Copy the video stream without re-encoding
    '-c:a', 'aac',    # Encode audio to AAC
    '-map', '0:v:0',  # Use the first video stream from the first input
    '-map', '1:a:0',  # Use the first audio stream from the second input
    new_video_file
]

print("Combining video and audio...")
process = subprocess.run(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

if process.returncode != 0:
    print("FFmpeg encountered an error:")
    print(process.stderr)
else:
    print(f"New video created: {new_video_file}")

# Verify that the output video exists
if os.path.exists(new_video_file):
    print(f"{new_video_file} has been successfully created.")
else:
    print(f"Error: {new_video_file} was not created.")

"""# Posting a reel"""

!pip install instagrapi

from instagrapi import Client
import os

# Initialize and login
cl = Client()
username = 'brainrot.center'
password = 'brainrot.center1234'

try:
    cl.login(username, password)
except Exception as e:
    print(f"Login failed: {e}")
    exit(1)

# Path to your Reel video
video_path = '/content/new_final_video.mp4'

# Reel caption
caption = "havent posted in over a month mb. oh wow, you like pink floyd? congrats on sitting through 20 full minutes of 50 year old whale noises and pretentious guitar snoozefests. nothing says 'Im intellectually superior' quite like zoning out to grandpas 'experimental' psychedelic lullabies while pretending youve unlocked lifes secrets. Oh, is that the genius of roger waters profound lyrics or is it just another 5-minute long keyboard chord? stop acting like deciphering Floyd lyrics is your phd thesis. go touch grass instead of worshipping album covers painted by a bored art student. youre not deep, just annoying. #ai #brainrot #reddit #fyp #aita #aitah"
usertags = ["ai", "brainrot", "reddit", "fyp"]

# Upload the Reel
try:
    media = media = cl.clip_upload(path=video_path, caption=caption)
    print(f"Reel posted successfully! Media ID: {media.dict()['id']}")
except Exception as e:
    print(f"An error occurred while uploading the Reel: {e}")

"""# Posting onto YT Shorts"""

!pip install google-api-python-client

import os
import random
import time
import httplib2
import argparse
from apiclient.discovery import build
from apiclient.errors import HttpError
from apiclient.http import MediaFileUpload
from oauth2client.client import flow_from_clientsecrets
from oauth2client.file import Storage
from oauth2client.tools import run_flow

CLIENT_SECRETS_FILE = "/content/drive/MyDrive/Personal_stuff/brainrot/client_web.json"
YOUTUBE_UPLOAD_SCOPE = "https://www.googleapis.com/auth/youtube.upload"
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"
VALID_PRIVACY_STATUSES = ("public", "private", "unlisted")
MAX_RETRIES = 10

# Simulate command-line arguments
class Args:
    def __init__(self):
        self.file = "/content/new_final_video.mp4"  # Update with your video file path
        self.title = "Self-tuned AI Generated Brainrot Content"
        self.description = "Dutch's final speech copypasta: I got a plan John. This is a good one. We can’t always fight nature, John. We can’t fight change. We can’t fight gravity. We can’t fight nothing. My whole life, all I ever did was fight. But I can’t give up, neither. I can’t fight my own nature. That’s a paradox, John. You see? When I’m gone, they’ll just find another monster. They have to, because they have to justify their wages. Our time has passed, John."
        self.category = "27"  # Example: 22 = People & Blogs
        self.keywords = "ai,brainrot,reddit,fyp"
        self.privacyStatus = "public"

args = Args()

def get_authenticated_service():
    flow = flow_from_clientsecrets(CLIENT_SECRETS_FILE, scope=YOUTUBE_UPLOAD_SCOPE)
    storage = Storage(f"{os.path.splitext(CLIENT_SECRETS_FILE)[0]}-oauth2.json")
    credentials = storage.get()

    if not credentials or credentials.invalid:
        flags = argparse.Namespace(
            auth_host_name='localhost',
            auth_host_port=[8080, 8090],
            logging_level='ERROR',
            noauth_local_webserver=True,
        )
        credentials = run_flow(flow, storage, flags=flags)

    return build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, credentials=credentials)

def initialize_upload(youtube, options):
    tags = options.keywords.split(",") if options.keywords else None
    body = {
        "snippet": {
            "title": options.title,
            "description": options.description,
            "tags": tags,
            "categoryId": options.category
        },
        "status": {
            "privacyStatus": options.privacyStatus
        }
    }

    insert_request = youtube.videos().insert(
        part=",".join(body.keys()),
        body=body,
        media_body=MediaFileUpload(options.file, chunksize=-1, resumable=True)
    )
    resumable_upload(insert_request)

def resumable_upload(insert_request):
    response = None
    error = None
    retry = 0
    while response is None:
        try:
            print("Uploading file...")
            status, response = insert_request.next_chunk()
            if response is not None:
                if "id" in response:
                    print(f"Video id '{response['id']}' was successfully uploaded.")
                    return
                else:
                    raise Exception(f"Unexpected response: {response}")
        except HttpError as e:
            if e.resp.status in [500, 502, 503, 504]:
                error = f"A retriable HTTP error {e.resp.status} occurred:\n{e.content}"
            else:
                raise
        except (httplib2.HttpLib2Error, IOError) as e:
            error = f"A retriable error occurred: {e}"

        if error:
            print(error)
            retry += 1
            if retry > MAX_RETRIES:
                raise Exception("No longer attempting to retry.")
            sleep_seconds = random.random() * (2 ** retry)
            print(f"Sleeping {sleep_seconds:.2f} seconds and retrying...")
            time.sleep(sleep_seconds  )

if __name__ == "__main__":
    if not os.path.exists(args.file):
        raise FileNotFoundError(f"Video file '{args.file}' not found.")

    youtube = get_authenticated_service()
    try:
        initialize_upload(youtube, args)
    except HttpError as e:
        print(f"An HTTP error {e.resp.status} occurred:\n{e.content}")